# -*- coding: utf-8 -*-
"""
Created on Wed Mar  3 09:22:52 2021

@author: hugod
"""

#TODO
# pip install fuzzy-c-means
#from fcmeans import FCM

import pandas as pd
import numpy as np
import os
import seaborn as sns
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import hdbscan
from scipy.signal import savgol_filter

path = "../ComputedClusters"
if not os.path.exists(path):
    os.mkdir(path)

df = pd.read_csv('../csv/players_stats.csv')

#print(df.shape)
#df = df.dropna()
#print(df)
data = np.zeros((df.shape[0],8))
#print(df[['2P%', '3P%']].values[0])

for i in range(df.shape[0]):
    data[i] = df[['TRB', 'PTS', 'AST', 'DWS', 'TS%', "3PA", "OWS","USG%"]].values[i]
    

# Compute DBSCAN

def generate_Computed_Clusters_Directory():
    X = StandardScaler().fit_transform(data)
    i = 0.1
    j = 2
    while i <= 1:
        while j <= 10:
            db = DBSCAN(eps=i, min_samples=j).fit(X)
            core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
            core_samples_mask[db.core_sample_indices_] = True
            labels = db.labels_
            #print(labels)
            # Number of clusters in labels, ignoring noise if present.
            n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise_ = list(labels).count(-1)
            noise_prop = n_noise_/len(labels)
            if(noise_prop < 0.7):
                res = np.zeros((0,2))
                result = pd.DataFrame(res)
                for k in range(df.shape[0]):
                    row = [[df['Player'].values[k], labels[k]]]
                    result = result.append(row)
                result = result.sort_values(by=[1], ascending = False)
                pathfile = path+"/epsilon_"+str(round(i,2))+"_MinPoints_"+str(j)+"_NoiseProp_"+str(round(noise_prop,2))+".csv"
                result.to_csv(pathfile, index =False, sep=';')
            j+=1
        j=2
        i+=0.1

#generate_Computed_Clusters_Directory()

def iterative_Clustering(epsilon,minPoints,noise_prop):
    # first we retrieve all the csv file generated by the previous function
    reference = pd.read_csv('../csv/players_stats.csv')
    path = "epsilon_"+str(epsilon)+"_MinPoints_"+str(minPoints)+"_NoiseProp_"+str(noise_prop)+".csv"
    df = pd.read_csv('./ComputedClusters/'+path, delimiter=';')
    tmp = pd.DataFrame()
    #then we are going 
    for k in range(df.shape[0]):
        if (df['1'].values[k] == 0 or df['1'].values[k] == -1):
            row = [[df['0'].values[k]]]
            tmp = tmp.append(row)
    player_stats = pd.DataFrame();
    for i in range(tmp.shape[0]):
        for j in range(reference.shape[0]):
            if(tmp[0].values[i] == reference['Player'].values[j]):
                row = [reference[['TRB', 'PTS', 'AST', 'DWS', 'TS%', "3PA", "OWS","USG%"]].values[j]]
                player_stats = player_stats.append(row)
    #print(player_stats)
    X = StandardScaler().fit_transform(player_stats)
    db = DBSCAN(eps=0.9, min_samples=3).fit(X)
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True
    labels = db.labels_
    #print(labels)
    # Number of clusters in labels, ignoring noise if present.
    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
    n_noise_ = list(labels).count(-1)
    noise_prop = n_noise_/len(labels)
    res = np.zeros((0,2))
    result = pd.DataFrame(res)
    for k in range(player_stats.shape[0]):
        row = [[reference['Player'].values[k], labels[k]]]
        result = result.append(row)
        result = result.sort_values(by=[1], ascending = False)
    print(result)
    
#iterative_Clustering(0.9,2,0.52)



    
    

    
"""
def soft_clustering_HDBSCAN():
    clusterer = hdbscan.HDBSCAN(min_cluster_size=2, cluster_selection_epsilon=0.9).fit_predict(data)
    result = pd.DataFrame(clusterer)
    result = result.sort_values(by=[0], ascending = False)
    
    return result
    

#result = soft_clustering_HDBSCAN()
    

from sklearn import datasets
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

digits = datasets.load_digits()
#data = digits.data
print(data)
projection = TSNE().fit_transform(data)
plt.scatter(*projection.T)
import hdbscan
clusterer = hdbscan.HDBSCAN(min_cluster_size=5, prediction_data=True).fit(data)
color_palette = sns.color_palette('Paired', 12)
cluster_colors = [color_palette[x] if x >= 0
                  else (0.5, 0.5, 0.5)
                  for x in clusterer.labels_]
cluster_member_colors = [sns.desaturate(x, p) for x, p in
                         zip(cluster_colors, clusterer.probabilities_)]
plt.scatter(*projection.T, s=50, linewidth=0, c=cluster_member_colors, alpha=0.25)
"""
import hdbscan
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import matplotlib as mpl

from scipy.spatial.distance import cdist


sns.set_context('poster')
sns.set_style('white')
sns.set_color_codes()

plot_kwds={'alpha':0.25, 's':60, 'linewidths':0}
palette = sns.color_palette('deep', 12)
fig = plt.figure()
ax = fig.add_subplot(111)
plt.scatter(data[:,0], data[:,1], **plot_kwds)
ax.set_xticks([])
ax.set_yticks([]);
clusterer = hdbscan.HDBSCAN(min_cluster_size=2, cluster_selection_epsilon=0.22).fit(data)

pal = sns.color_palette('deep', 8)
colors = [sns.desaturate(pal[col], sat) for col, sat in zip(clusterer.labels_,
                                                            clusterer.probabilities_)]
print(data.columns)
plt.scatter(data[:,0], data[:,1], c=colors, **plot_kwds);
#clusterer.single_linkage_tree_.plot(cmap='viridis', colorbar=True)